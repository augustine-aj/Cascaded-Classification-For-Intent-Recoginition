{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e1f7a3-53fc-43b1-9f5b-42109a3ad4ed",
   "metadata": {},
   "source": [
    "# Introduction to Cascaded Classification System for Intent Recognition\n",
    "\n",
    "#### In the realm of natural language processing (NLP), effective intent recognition plays a pivotal role in enhancing user interaction with chatbots, virtual assistants, and automated customer support systems. This project implements a two-stage classification system that categorizes user queries into distinct intents based on their content.\n",
    "\n",
    "#### The classification process is divided into two stages:\n",
    "\n",
    "#### ****1. Symptom Checker vs. Non-Symptom Checker****: In the initial stage, a binary classifier, utilizing the BERT architecture, determines whether a given query pertains to a \"Symptom Checker\" or a \"Non-Symptom Checker.\" This differentiation allows the system to streamline the subsequent classification process.\n",
    "\n",
    "#### ****2. Intent Classification****: If a query is classified as \"Non-Symptom Checker,\" a second classifier further categorizes it into specific intents, such as \"Treatment Information,\" \"Patient Support,\" \"FAQs,\" and \"Appointment Scheduling.\" This hierarchical approach enables the model to provide more accurate and contextually relevant responses to user inquiries.\n",
    "\n",
    "#### The project leverages the BERT (Bidirectional Encoder Representations from Transformers) model for its powerful contextual understanding, ensuring high accuracy in intent classification. By utilizing this two-stage classification system, the project aims to improve user experience in healthcare-related applications by efficiently addressing queries based on their specific intent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408fb46-3455-452e-9161-13e6c3b40909",
   "metadata": {},
   "source": [
    "#### **********************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81502422-f745-4cf0-bdc2-71a589ae08c3",
   "metadata": {},
   "source": [
    "### Imports the Required Libraries\r\n",
    "\r\n",
    "- **transformers**: For using BERT for tokenization and model building.\r\n",
    "- **torch**: For tensor operations and model training.\r\n",
    "- **pandas**: For data manipulation and reading CSV files.\r\n",
    "- **sklearn**: For splitting datasets and evaluating models.\r\n",
    "- **numpy**: For numerical operations.\r\n",
    "- **joblib**: For saving model ojects.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55245a39-3b94-41a8-ae69-0c0097852435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efa0e0-f36d-45b4-bc98-86735698ba4c",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "loads the dataset containing queries and their corresponding intents into a Pandas DataFrame. The file path should be adjusted according to your directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2df237-a7a2-4668-88ee-8b0cfcc5348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/augus/PycharmProjects/image_captioning/RAG DEMO/faq_queries_intents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe26920-6536-4b9d-aed8-11ba886c05b4",
   "metadata": {},
   "source": [
    "### Label Encoding for the Intents\n",
    "\n",
    "The following line encodes the intents into numeric labels using `pd.factorize()`, which assigns a unique integer to each unique intent. This creates a new column `label` for machine learning purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049a7e10-8ae9-439b-8828-8005514f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'], intent_labels = pd.factorize(df['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72c76d-ef9e-459f-bcec-4cfeb9c0a3da",
   "metadata": {},
   "source": [
    "####  Define Symptom Checker and Non-Symptom Checker Categories\n",
    "\n",
    "Here, two categories for classification are defined:\n",
    "\n",
    "- **Symptom Checker** : Contains queries related to symptoms.\n",
    "- **Non-Symptom Checker** : Contains other types of queries.\n",
    "A new binary label is created that assigns 0 for symptom checker intents and 1 for non-symptom checker intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c31607c-8959-4746-b1e7-845ddde895bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_checker = ['Symptom_Checker']\n",
    "non_symptom_checker = ['Treatment_Information', 'Patient_Support', 'FAQs', 'Appointment_Scheduling']\n",
    "\n",
    "# Binary classification: Symptom Checker vs. Non-Symptom Checker\n",
    "df['binary_label'] = df['intent'].apply(lambda x: 0 if x in symptom_checker else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f7cb7-9e1b-438e-9978-2e2f950de304",
   "metadata": {},
   "source": [
    "#### Split the Dataset \n",
    "\n",
    " splits the dataset into training and testing sets for the binary classification model. The test size is set to 20%, and a random state is specified for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26449c20-f34a-4424-892f-d12163bbed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_route, X_test_route, y_train_route, y_test_route = train_test_split(df['query'], df['binary_label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9bf4e-1953-4e6b-958e-7ff28ba9e018",
   "metadata": {},
   "source": [
    "####  Load BERT Tokenizer and Model for Binary Classification\n",
    "\n",
    "The BERT tokenizer and model are loaded from the Hugging Face Transformers library. The model is configured for binary classification with `num_labels=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b6ba31-a2d5-4b0c-8c44-4e2ea8e7a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_route = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_route = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053abe6-f712-460b-becb-4b3f042f52e4",
   "metadata": {},
   "source": [
    "#### Check for GPU Availability\n",
    "\n",
    "checks if a GPU is available for faster training and moves the model to the appropriate device (CPU or GPU). The tokenizer and model are then saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0771f324-77b1-44fb-881b-630e31260766",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_route.to(device)\n",
    "\n",
    "# Save tokenizer and model after training (optional)\n",
    "tokenizer_route.save_pretrained(\"route_tokenizer\")\n",
    "model_route.save_pretrained(\"route_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56facf3-0b29-4005-bdf8-419a5b57f5cb",
   "metadata": {},
   "source": [
    "#### Filter Out Non-Symptom Checker Data\n",
    "\n",
    "filters the DataFrame to get only non-symptom checker data and applies label encoding. It then splits the non-symptom checker data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f51e19d3-7eea-4f9c-a5f0-ade202e5ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_symptom_df = df[df['binary_label'] == 1]\n",
    "non_symptom_df['label'], non_symptom_labels = pd.factorize(non_symptom_df['intent'])\n",
    "\n",
    "# Split non-symptom checker data\n",
    "X_train_non, X_test_non, y_train_non, y_test_non = train_test_split(non_symptom_df['query'], non_symptom_df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975ddc8-4a0a-4008-812d-6a9faafb8ace",
   "metadata": {},
   "source": [
    "#### Load BERT Tokenizer and Model for Non-Symptom Checker Classification\n",
    "\n",
    "Similar to the previous tokenizer and model setup, this block loads a new BERT tokenizer and model for the non-symptom checker classification task. The number of labels corresponds to the unique intents in the non-symptom checker category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ae0621-f39b-404c-bfcf-52d9aa9d3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_non_symptom = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_non_symptom = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(non_symptom_labels))\n",
    "\n",
    "tokenizer_non_symptom.save_pretrained(\"non_symptom_tokenizer\")\n",
    "model_non_symptom.save_pretrained(\"non_symptom_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10516de-f402-4d2c-95be-974f297b4dd4",
   "metadata": {},
   "source": [
    "#### Define the Training Function\n",
    "\n",
    "The function `train_bert_model` trains a BERT model using the provided training data. The process includes:\n",
    "\n",
    "1. **Initializing the Optimizer and Loss Function**:\n",
    "   - The AdamW optimizer and the cross-entropy loss function are initialized to optimize the model's parameters during training.\n",
    "\n",
    "2. **Iterating through the Dataset**:\n",
    "   - The training process involves iterating through the dataset for a specified number of epochs, allowing the model to learn from the data over multiple passes.\n",
    "\n",
    "3. **Dividing the Data into Batches**:\n",
    "   - The data is divided into batches for training, which helps in managing memory and improving training efficiency.\n",
    "\n",
    "4. **Tokenizing the Input Sentences**:\n",
    "   - Input sentences are tokenized using the BERT tokenizer, converting text into a format suitable for the model.\n",
    "\n",
    "5. **Performing Forward and Backward Passes**:\n",
    "   - A forward pass is conducted to compute the model's output and the loss. Subsequently, a backward pass updates the model's weights based on the computed loss.\n",
    "\n",
    "6. **Printing the Average Loss**:\n",
    "   - The average loss is printed after each epoch, providing feedback on the model's learning progress and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651906fb-1048-4ad1-9c06-5024f8ea040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(model, tokenizer, X_train, y_train, epochs=3, batch_size=16):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Convert y_train to a numpy array (this will fix the ValueError)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_sentences = X_train[i:i + batch_size]\n",
    "            batch_labels = torch.tensor(y_train[i:i + batch_size]).to(device)\n",
    "\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(batch_sentences.tolist(), return_tensors='pt', truncation=True, padding=True,\n",
    "                               max_length=64).to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs).logits\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed with average loss: {total_loss / len(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18161d-6676-4699-927c-13b7807a57a3",
   "metadata": {},
   "source": [
    "#### Train the Models\n",
    "\n",
    "call the training function for both the binary and non-symptom checker models using their respective training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd54194-a2c3-4561-9d45-f7570ccb8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed with average loss: 0.00019462801171206388\n",
      "Epoch 2 completed with average loss: 4.2668648287905795e-06\n",
      "Epoch 3 completed with average loss: 2.3692240212388344e-06\n",
      "Epoch 1 completed with average loss: 0.031784947188743105\n",
      "Epoch 2 completed with average loss: 0.013067292687814869\n",
      "Epoch 3 completed with average loss: 0.007467058579539413\n"
     ]
    }
   ],
   "source": [
    "train_bert_model(model_route, tokenizer_route, X_train_route, y_train_route)\n",
    "\n",
    "train_bert_model(model_non_symptom, tokenizer_non_symptom, X_train_non, y_train_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561dd10-336b-4a48-abcc-7a7b84e14031",
   "metadata": {},
   "source": [
    "#### Save the Trained Models and Tokenizers\n",
    "\n",
    "After training, the models and tokenizers are saved for later use. This allows you to avoid retraining when making predictions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2859658-66f8-4fc3-a62e-85371a60a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_route.save_pretrained(\"route_tokenizer\")\n",
    "model_route.save_pretrained(\"route_model\")\n",
    "\n",
    "tokenizer_non_symptom.save_pretrained(\"non_symptom_tokenizer\")\n",
    "model_non_symptom.save_pretrained(\"non_symptom_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb447182-8b9e-4b67-811c-c91544796a92",
   "metadata": {},
   "source": [
    "#### Reload the Models and Tokenizers for Inference\n",
    "\n",
    "This block reloads the saved models and tokenizers for use in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b063fc6-23ff-44e6-8210-b02b62cc9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_route = BertTokenizer.from_pretrained(\"route_tokenizer\")\n",
    "model_route = BertForSequenceClassification.from_pretrained(\"route_model\").to(device)\n",
    "\n",
    "tokenizer_non_symptom = BertTokenizer.from_pretrained(\"non_symptom_tokenizer\")\n",
    "model_non_symptom = BertForSequenceClassification.from_pretrained(\"non_symptom_model\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9def466-e348-46ca-a5cc-bf19aab03ac6",
   "metadata": {},
   "source": [
    "#### Define the Prediction Function\n",
    "\n",
    "This function predicts the intent of a given sentence by first checking if it belongs to the symptom checker category. If it does, it returns \"Symptom_Checker\"; otherwise, it predicts the specific intent from the non-symptom checker category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40bf650e-889d-4817-bb71-1912e25a0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(sentence):\n",
    "    model_route.eval()\n",
    "    model_non_symptom.eval()\n",
    "\n",
    "    # Tokenize and predict using the routing classifier (binary)\n",
    "    inputs_route = tokenizer_route(sentence, return_tensors='pt', truncation=True, padding=True, max_length=64).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits_route = model_route(**inputs_route).logits\n",
    "    predicted_binary_label = torch.argmax(logits_route, dim=1).cpu().item()\n",
    "\n",
    "    if predicted_binary_label == 0:\n",
    "        return \"Symptom_Checker\"\n",
    "    else:\n",
    "        # Predict using the non-symptom classifier (multiclass)\n",
    "        inputs_non_symptom = tokenizer_non_symptom(sentence, return_tensors='pt', truncation=True, padding=True, max_length=64).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits_non_symptom = model_non_symptom(**inputs_non_symptom).logits\n",
    "        predicted_label = torch.argmax(logits_non_symptom, dim=1).cpu().item()\n",
    "        return non_symptom_labels[predicted_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a241d-c0ab-4e30-97d3-c1bf9eee0327",
   "metadata": {},
   "source": [
    "#### Test the Model with Sample Queries\n",
    "\n",
    "This block tests the model by passing sample queries through the prediction function and printing the predicted intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90842d6b-5cf7-4ed8-9723-3d24156f1c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the symptoms of flu?' -> Predicted Intent: Treatment_Information\n",
      "Query: 'How can I book an appointment?' -> Predicted Intent: appointment_scheduling\n",
      "Query: 'What treatment is available for diabetes?' -> Predicted Intent: Treatment_Information\n",
      "Query: 'Can you help me with my insurance questions?' -> Predicted Intent: Patient_Support\n"
     ]
    }
   ],
   "source": [
    "sample_queries = [\n",
    "    \"What are the symptoms of flu?\",\n",
    "    \"How can I book an appointment?\",\n",
    "    \"What treatment is available for diabetes?\",\n",
    "    \"Can you help me with my insurance questions?\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"Query: '{query}' -> Predicted Intent: {predict_intent(query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b866b-20d5-4295-96b4-9fb9ad1c9275",
   "metadata": {},
   "source": [
    "#### Evaluating Predictions with Accuracy and F1-Score\n",
    "\n",
    "\r\n",
    "After making predictions, we evaluate them using **Accuracy** and **F1-Score**:\r\n",
    "\r\n",
    "- **Accuracy**:\r\n",
    "  - Measures the percentage of correct predictions made by the model.\r\n",
    "\r\n",
    "- **F1-Score**:\r\n",
    "  - Combines precision and recall into a single metric, making it useful for assessing model performance on imbalanced datasets.\r\n",
    "\r\n",
    "The `classification_report` function provides detailed metrics for both classes, including:\r\n",
    "- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\r\n",
    "- **Recall**: The ratio of correctly predicted positive observations to the all observations in actual class.\r\n",
    "- **F1-Score**: The weighted average of precision and recall, providing a balance between the two.\r\n",
    "\r\n",
    "This comprehensive evaluation helps in understanding the model's strengths and weaknesses across different classes.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e5592-661f-40c6-ac58-f295ef93e3f1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we implemented a two-stage intent recognition system using BERT, capable of classifying user queries into symptom checker and non-symptom checker categories. The models were trained on a dataset of queries and intents, and their performance can be evaluated using further metrics as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
